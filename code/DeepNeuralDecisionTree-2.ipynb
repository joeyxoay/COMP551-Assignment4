{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Neural Decision Tree"
      ],
      "metadata": {
        "id": "r42zGEo7AFGG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.compose import make_column_transformer\n",
        "from sklearn.compose import ColumnTransformer"
      ],
      "metadata": {
        "id": "XQZ7Vu_ji4pl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Decision Nodes\n",
        "def create_decision_node(model_output, threshold=0.5):\n",
        "    decisions = np.where(model_output > threshold, 1, 0)\n",
        "    return decisions\n",
        "\n",
        "#Train the Model\n",
        "def train_model(model, x_train, y_train):\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    model.fit(x_train, y_train, epochs=100, batch_size=10, verbose=0)\n",
        "\n",
        "# Prediction\n",
        "def predict(model, x_test):\n",
        "    model_output = model.predict(x_test)\n",
        "    decisions = create_decision_node(model_output)\n",
        "    return decisions"
      ],
      "metadata": {
        "id": "kNRiv9nrqdd7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#IRIS dataset\n",
        "\n",
        "# Load the Iris dataset\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\n",
        "columns = ['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm', 'Species']\n",
        "iris = pd.read_csv(url, names=columns)\n",
        "\n",
        "# Preprocess\n",
        "X = iris.drop('Species', axis=1)\n",
        "y = iris['Species']\n",
        "\n",
        "# Encode\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "y_categorical = to_categorical(y_encoded)\n",
        "\n",
        "# Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_categorical, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train\n",
        "model = Sequential([\n",
        "        Dense(64, activation='relu', input_shape=(4,)),\n",
        "        Dense(32, activation='relu'),\n",
        "        Dense(3, activation='softmax')\n",
        "    ])\n",
        "\n",
        "train_model(model, X_train, y_train)\n",
        "\n",
        "#Evaluate\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N-25EsxFq9l6",
        "outputId": "b6e5db30-23f8-49b5-94e3-0d470b60177a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 136ms/step - loss: 0.0698 - accuracy: 0.9667\n",
            "Accuracy: 96.67%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Habermanâ€™s Survival dataset\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/haberman/haberman.data\"\n",
        "column = [\"age\", \"year_of_treatment\", \"positive_lymph_nodes\", \"survival_status_after_5_years\"]\n",
        "haberman = pd.read_csv(url, names=column)\n",
        "\n",
        "# Preprocess\n",
        "X = haberman.drop(\"survival_status_after_5_years\", axis=1)\n",
        "y = haberman[\"survival_status_after_5_years\"]\n",
        "\n",
        "# Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train\n",
        "model = Sequential([\n",
        "        Dense(64, activation='relu', input_shape=(3,)),\n",
        "        Dense(32, activation='relu'),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "train_model(model, X_train, y_train)\n",
        "\n",
        "#Evaluate\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-h79G-4JsEP0",
        "outputId": "c46a09b0-a011-4fc3-b05a-bb6c5a8fd5a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py:1260: SyntaxWarning: In loss categorical_crossentropy, expected y_pred.shape to be (batch_size, num_classes) with num_classes > 1. Received: y_pred.shape=(None, 1). Consider using 'binary_crossentropy' if you only have 2 classes.\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - accuracy: 0.7097\n",
            "Accuracy: 70.97%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py:1260: SyntaxWarning: In loss categorical_crossentropy, expected y_pred.shape to be (batch_size, num_classes) with num_classes > 1. Received: y_pred.shape=(None, 1). Consider using 'binary_crossentropy' if you only have 2 classes.\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Car Evaluation dataset\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data\"\n",
        "column = [\"buying_price\", \"maintenance_cost\", \"number_of_doors\", \"number_of_persons\", \"lug_boot\", \"safety\", \"decision\"]\n",
        "car = pd.read_csv(url, names=column)\n",
        "\n",
        "# Preprocess\n",
        "X = car.drop(\"decision\", axis=1)\n",
        "y = car[\"decision\"]\n",
        "\n",
        "# Encode\n",
        "one_hot_encoder = OneHotEncoder()\n",
        "X_encoded = one_hot_encoder.fit_transform(X).toarray()\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y_encoded, test_size=0.2)\n",
        "\n",
        "# Encode\n",
        "y_train_encoded = to_categorical(y_train)\n",
        "y_test_encoded = to_categorical(y_test)\n",
        "\n",
        "# Train\n",
        "model = Sequential([\n",
        "        Dense(64, activation='relu', input_shape=(21,)),\n",
        "        Dense(32, activation='relu'),\n",
        "        Dense(4, activation='softmax')\n",
        "    ])\n",
        "\n",
        "train_model(model, X_train, y_train_encoded)\n",
        "\n",
        "#Evaluate\n",
        "loss, accuracy = model.evaluate(X_test, y_test_encoded)\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wLomzbODwEJr",
        "outputId": "32902ad4-157c-4118-a199-271d08f754a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11/11 [==============================] - 0s 3ms/step - loss: 1.0170e-04 - accuracy: 1.0000\n",
            "Accuracy: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Breast Cancer Wisconsin\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data\"\n",
        "column = [\"Sample_code_number\", \"Clump_thickness\", \"Uniformity_of_cell_size\", \"Uniformity_of_cell_shape\", \"Marginal_adhesion\", \"Single_epithelial_cell_size\t\", \"Bare_nuclei\", \"Bland_chromatin\",\"Normal_nucleoli\", \"Mitoses\", \"Class\"]\n",
        "cancer = pd.read_csv(url, names=column)\n",
        "\n",
        "# Preprocess\n",
        "cancer.replace(\"?\", np.nan, inplace=True)\n",
        "cancer.dropna(inplace=True)\n",
        "\n",
        "X = cancer.drop(\"Class\", axis=1)\n",
        "y = cancer[\"Class\"]\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "#Encode\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "y_encoded = to_categorical(y_encoded)\n",
        "\n",
        "#Split\n",
        "X_train, X_test, y_train_encoded, y_test_encoded = train_test_split(X_scaled, y_encoded, test_size=0.2)\n",
        "\n",
        "# Train\n",
        "model = Sequential([\n",
        "        Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "        Dense(32, activation='relu'),\n",
        "        Dense(y_encoded.shape[1], activation='softmax')\n",
        "    ])\n",
        "train_model(model, X_train, y_train_encoded)\n",
        "\n",
        "# Evaluate\n",
        "loss, accuracy = model.evaluate(X_test, y_test_encoded)\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W1a1zEEIZ4WJ",
        "outputId": "8176c1e4-62a1-410d-def1-6bba0fb2fe77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5/5 [==============================] - 0s 3ms/step - loss: 0.2758 - accuracy: 0.9635\n",
            "Accuracy: 96.35%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Poker Hands\n",
        "url_train = \"https://archive.ics.uci.edu/ml/machine-learning-databases/poker/poker-hand-training-true.data\"\n",
        "url_test = \"https://archive.ics.uci.edu/ml/machine-learning-databases/poker/poker-hand-testing.data\"\n",
        "column = [\"S1\", \"C1\", \"S2\", \"C2\", \"S3\", \"C3\", \"S4\", \"C4\", \"S5\", \"C5\", \"Class\"]\n",
        "poker_train = pd.read_csv(url_train, names=column)\n",
        "poker_est = pd.read_csv(url_test, names=column)\n",
        "\n",
        "#Train Test\n",
        "X_train = poker_train.drop(\"Class\", axis=1)\n",
        "y_train = poker_train[\"Class\"]\n",
        "X_test = poker_test.drop(\"Class\", axis=1)\n",
        "y_test = poker_test[\"Class\"]\n",
        "\n",
        "#Encode\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "y_test_encoded = label_encoder.transform(y_test)\n",
        "\n",
        "one_hot_encoder = OneHotEncoder(categories='auto')\n",
        "y_train_encoded = one_hot_encoder.fit_transform(np.array(y_train_encoded).reshape(-1, 1)).toarray()\n",
        "y_test_encoded = one_hot_encoder.transform(np.array(y_test_encoded).reshape(-1, 1)).toarray()\n",
        "\n",
        "num_classes = y_train_encoded.shape[1]\n",
        "\n",
        "# Train\n",
        "model = Sequential([\n",
        "        Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "        Dense(32, activation='relu'),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "train_model(model, X_train, y_train_encoded)\n",
        "\n",
        "# Evaluate\n",
        "loss, accuracy = model.evaluate(X_test, y_test_encoded)\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j7kLX3X5ddhi",
        "outputId": "3bd0143a-fe99-4a5c-9500-fd97f2dcf6dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "31250/31250 [==============================] - 73s 2ms/step - loss: 0.0934 - accuracy: 0.9813\n",
            "Accuracy: 98.13%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#German Credit Dataset\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.data\"\n",
        "column = [\"Status of existing checking account\", \"Duration\", \"Credit history\", \"Purpose\", \"Credit amount\", \"Savings account/bonds\", \"Present employment since\", \"Installment rate in percentage of disposable income\", \"Personal status and sex\", \"Other debtors / guarantors\", \"Present residence since\", \"Property\", \"Age\", \"Other installment plans\", \"Housing\", \"Number of existing credits at this bank\", \"Telephone\", \"Foreign worker\", \"Credit risk\"]\n",
        "credit = pd.read_csv(url, names=column, sep=' ', header=None)\n",
        "\n",
        "# Identify categorical and numerical columns\n",
        "categorical_cols = [\"Status of existing checking account\", \"Credit history\", \"Purpose\", \"Savings account/bonds\",\"Present employment since\", \"Personal status and sex\", \"Other debtors / guarantors\", \"Property\", \"Other installment plans\", \"Housing\", \"Job\", \"Telephone\", \"Foreign worker\"]\n",
        "numerical_cols = [\"Duration\", \"Credit amount\", \"Installment rate in percentage of disposable income\",\"Present residence since\", \"Age\", \"Number of existing credits at this bank\", \"Number of people being liable to provide maintenance for\"]\n",
        "\n",
        "# Preprocess the data\n",
        "X = credit.drop(\"Credit risk\", axis=1)\n",
        "y = credit[\"Credit risk\"]\n",
        "\n",
        "numerical_transformer = StandardScaler()\n",
        "categorical_transformer = OneHotEncoder()\n",
        "\n",
        "# Create a column transformer for preprocessing\n",
        "preprocessor = make_column_transformer(\n",
        "    (numerical_transformer, numerical_cols),\n",
        "    (categorical_transformer, categorical_cols)\n",
        ")\n",
        "\n",
        "X_processed = preprocessor.fit_transform(X)\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=0.2)\n",
        "\n",
        "model = Sequential([\n",
        "        Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "        Dense(32, activation='relu'),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "# Train the model and evaluate its accuracy\n",
        "train_model(model, X_train, y_train_encoded)\n",
        "\n",
        "# Evaluate\n",
        "loss, accuracy = model.evaluate(X_test, y_test_encoded)\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5neNhJfwiDGA",
        "outputId": "39bca96e-2848-4d96-9abc-da366f707162"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7/7 [==============================] - 0s 4ms/step - loss: 2.5119 - accuracy: 0.5450\n",
            "Accuracy: 54.50%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# **Experiment with different hyperparameters**"
      ],
      "metadata": {
        "id": "N8k3R626Rh2a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Number of Batch-Size**"
      ],
      "metadata": {
        "id": "g7tpo-sMR9Pc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### LARGE NUMBER OF Batch Size\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.compose import make_column_transformer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "def create_decision_node(model_output, threshold=0.5):\n",
        "    decisions = np.where(model_output > threshold, 1, 0)\n",
        "    return decisions\n",
        "\n",
        "#Train the Model\n",
        "def train_model(model, x_train, y_train):\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    model.fit(x_train, y_train, epochs=100, batch_size=50, verbose=0)\n",
        "\n",
        "# Prediction\n",
        "def predict(model, x_test):\n",
        "    model_output = model.predict(x_test)\n",
        "    decisions = create_decision_node(model_output)\n",
        "    return decisions\n",
        "\n",
        "\n",
        "#IRIS dataset\n",
        "\n",
        "# Load the Iris dataset\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\n",
        "columns = ['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm', 'Species']\n",
        "iris = pd.read_csv(url, names=columns)\n",
        "\n",
        "# Preprocess\n",
        "X = iris.drop('Species', axis=1)\n",
        "y = iris['Species']\n",
        "\n",
        "# Encode\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "y_categorical = to_categorical(y_encoded)\n",
        "\n",
        "# Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_categorical, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train\n",
        "model = Sequential([\n",
        "        Dense(64, activation='relu', input_shape=(4,)),\n",
        "        Dense(32, activation='relu'),\n",
        "        Dense(3, activation='softmax')\n",
        "    ])\n",
        "\n",
        "train_model(model, X_train, y_train)\n",
        "\n",
        "#Evaluate\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R3JLw8akRkf9",
        "outputId": "72206477-87cb-44a3-a629-8245aa6b2826"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 124ms/step - loss: 0.1219 - accuracy: 0.9667\n",
            "Accuracy: 96.67%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### SMALL NUMBER OF Batch Size\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.compose import make_column_transformer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "def create_decision_node(model_output, threshold=0.5):\n",
        "    decisions = np.where(model_output > threshold, 1, 0)\n",
        "    return decisions\n",
        "\n",
        "#Train the Model\n",
        "def train_model(model, x_train, y_train):\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    model.fit(x_train, y_train, epochs=100, batch_size=5, verbose=0)\n",
        "\n",
        "# Prediction\n",
        "def predict(model, x_test):\n",
        "    model_output = model.predict(x_test)\n",
        "    decisions = create_decision_node(model_output)\n",
        "    return decisions\n",
        "\n",
        "\n",
        "#IRIS dataset\n",
        "\n",
        "# Load the Iris dataset\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\n",
        "columns = ['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm', 'Species']\n",
        "iris = pd.read_csv(url, names=columns)\n",
        "\n",
        "# Preprocess\n",
        "X = iris.drop('Species', axis=1)\n",
        "y = iris['Species']\n",
        "\n",
        "# Encode\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "y_categorical = to_categorical(y_encoded)\n",
        "\n",
        "# Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_categorical, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train\n",
        "model = Sequential([\n",
        "        Dense(64, activation='relu', input_shape=(4,)),\n",
        "        Dense(32, activation='relu'),\n",
        "        Dense(3, activation='softmax')\n",
        "    ])\n",
        "\n",
        "train_model(model, X_train, y_train)\n",
        "\n",
        "#Evaluate\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LgZqnmz9ScmH",
        "outputId": "15c4756c-2f8d-4091-d782-259c403e0855"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 133ms/step - loss: 0.0631 - accuracy: 1.0000\n",
            "Accuracy: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Number of Epochs**"
      ],
      "metadata": {
        "id": "quqj6zA1TXJf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### SMALL NUMBER OF Epochs\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.compose import make_column_transformer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "def create_decision_node(model_output, threshold=0.5):\n",
        "    decisions = np.where(model_output > threshold, 1, 0)\n",
        "    return decisions\n",
        "\n",
        "#Train the Model\n",
        "def train_model(model, x_train, y_train):\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    model.fit(x_train, y_train, epochs=5, batch_size=10, verbose=0)\n",
        "\n",
        "# Prediction\n",
        "def predict(model, x_test):\n",
        "    model_output = model.predict(x_test)\n",
        "    decisions = create_decision_node(model_output)\n",
        "    return decisions\n",
        "\n",
        "\n",
        "#IRIS dataset\n",
        "\n",
        "# Load the Iris dataset\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\n",
        "columns = ['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm', 'Species']\n",
        "iris = pd.read_csv(url, names=columns)\n",
        "\n",
        "# Preprocess\n",
        "X = iris.drop('Species', axis=1)\n",
        "y = iris['Species']\n",
        "\n",
        "# Encode\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "y_categorical = to_categorical(y_encoded)\n",
        "\n",
        "# Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_categorical, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train\n",
        "model = Sequential([\n",
        "        Dense(64, activation='relu', input_shape=(4,)),\n",
        "        Dense(32, activation='relu'),\n",
        "        Dense(3, activation='softmax')\n",
        "    ])\n",
        "\n",
        "train_model(model, X_train, y_train)\n",
        "\n",
        "#Evaluate\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kpi0LhswTZBR",
        "outputId": "e755dd7e-73db-41f6-f052-af07e26b6961"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_test_function.<locals>.test_function at 0x7ea625b69900> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 186ms/step - loss: 0.5256 - accuracy: 0.8333\n",
            "Accuracy: 83.33%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### SMALL NUMBER OF Epochs\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.compose import make_column_transformer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "def create_decision_node(model_output, threshold=0.5):\n",
        "    decisions = np.where(model_output > threshold, 1, 0)\n",
        "    return decisions\n",
        "\n",
        "#Train the Model\n",
        "def train_model(model, x_train, y_train):\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    model.fit(x_train, y_train, epochs=8, batch_size=10, verbose=0)\n",
        "\n",
        "# Prediction\n",
        "def predict(model, x_test):\n",
        "    model_output = model.predict(x_test)\n",
        "    decisions = create_decision_node(model_output)\n",
        "    return decisions\n",
        "\n",
        "\n",
        "#IRIS dataset\n",
        "\n",
        "# Load the Iris dataset\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\n",
        "columns = ['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm', 'Species']\n",
        "iris = pd.read_csv(url, names=columns)\n",
        "\n",
        "# Preprocess\n",
        "X = iris.drop('Species', axis=1)\n",
        "y = iris['Species']\n",
        "\n",
        "# Encode\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "y_categorical = to_categorical(y_encoded)\n",
        "\n",
        "# Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_categorical, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train\n",
        "model = Sequential([\n",
        "        Dense(64, activation='relu', input_shape=(4,)),\n",
        "        Dense(32, activation='relu'),\n",
        "        Dense(3, activation='softmax')\n",
        "    ])\n",
        "\n",
        "train_model(model, X_train, y_train)\n",
        "\n",
        "#Evaluate\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MGrBzhnZTqRQ",
        "outputId": "09ef303b-c2c1-448a-e2ca-fa6b85bbb94c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 137ms/step - loss: 0.5425 - accuracy: 1.0000\n",
            "Accuracy: 100.00%\n"
          ]
        }
      ]
    }
  ]
}